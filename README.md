# CallReportParser â€” FIBOPlus

> A TypeScript + Vite utility for parsing bank Call Report PDFs, extracting text and structured data, and mapping to a financial ontology (FIBO + custom Call Report classes) for downstream vector/RAG workflows.

## ğŸš€ Overview  
This tool handles the full pipeline for analyzing bank Call Reports:

1. **PDF text extraction** â€” using the browser environment and `pdfjsLib` to read PDF files and extract pageâ€‘byâ€‘page text.  
2. **Chunking** â€” large document text is split into manageable pieces (default ~15,000 characters) to stay within LLM context limits.  
3. **Ontology mapping**  
   - *FIBO mapping*: Identify concepts, entities and relationships (e.g., financial institutions, instruments, risk events) and link them to a FIBO class.  
   - *Financial data extraction*: Extract specific numeric / percentage / date data points (assets, liabilities, income, risk metrics) and map them to a custom â€œCall Report Ontologyâ€ (prefix `crâ€‘`).  
4. **Deduplication** â€” Remove duplicate items (by name + class + context) so that the result set is clean and lean.  
5. **Structured JSON output** â€” Ready for ingestion into embedding/vector stores, search indices, RAG pipelines or downstream data systems.

## ğŸ§° Architecture & Key Files  
### PDF extraction  
- `pdfService.ts` â€” reads a `File` object (in the browser) using `FileReader`, then uses `pdfjsLib.getDocument(...)` to load the PDF, iterates pages, extracts text, and returns a `string`.  
- Assumes that `window.pdfjsLib` is available (for example via a `<script>` tag in `index.html`).  
### Ontology mapping  
- `services/geminiService.ts` â€” the core logic that:  
  - Accepts raw document text and a `onProgress` callback.  
  - Splits the text into chunks (using `chunkText`) of size defined by `CHUNK_SIZE` (default: 15,000 characters).  
  - For each chunk, prompts the Google GenAI model (via `@google/genai`) with a schema definition describing the output.  
  - Receives JSON output, parses into `FiboData`, and aggregates arrays: `concepts`, `entities`, `relationships`, `financial_data`.  
  - Deduplicates each array using composite keys (name/class/context or value/class/context).  
  - Returns a `FiboData` object (see interfaces below).  
### Types / Interfaces  
Defined in `types.ts` (or similar):  
```ts
export interface FiboItem {
  name: string;
  fibo_class: string;
  description: string;
  context: string;
}

export interface FinancialDataItem {
  name: string;
  value: string;
  ontology_class: string;
  description: string;
  context: string;
}

export interface FiboData {
  concepts: FiboItem[];
  entities: FiboItem[];
  relationships: FiboItem[];
  financial_data: FinancialDataItem[];
}
```
### Schema definitions (within the code)  
- `fiboItemSchema` â€” defines JSON schema for `FiboItem` objects: `{ name, fibo_class, description, context }`  
- `financialDataItemSchema` â€” defines JSON schema for `FinancialDataItem` objects: `{ name, value, ontology_class, description, context }`  
- `fiboSchema` â€” topâ€level schema for the entire output object (`FiboData`) containing arrays: `concepts`, `entities`, `relationships`, `financial_data`  
### Constants & Configuration  
- `CHUNK_SIZE = 15000` characters  
- `API_KEY` environment variable required (`process.env.API_KEY`) for the Google GenAI integration  
- Deduplication logic uses a `Map<string, â€¦>` keyed on composite string like `name|class|context` to eliminate duplicates.

## ğŸ§­ Getting Started  
### Prerequisites  
- Node.js (LTS recommended)  
- A valid Google GenAI API key stored as an environment variable named `API_KEY`:

```bash
export API_KEY="YOUR_GOOGLE_GENAI_KEY"
```

### Install  
```bash
npm install
# or
yarn install
```

### Run (development)  
```bash
npm run dev
```

### Example Usage  
```ts
import { extractTextFromPdf } from './utils/extractTextFromPdf';
import { applyFiboOntology } from './services/applyFiboOntology';

const file: File = /* obtained from fileâ€‘input */;
const text = await extractTextFromPdf(file);

const result = await applyFiboOntology(text, (msg) => console.log(msg));

console.log(JSON.stringify(result, null, 2));
```

## ğŸ“„ Output Schema  
The resulting JSON object has the shape defined by `FiboData`:

```json
{
  "concepts": [
    {
      "name": "Interest Rate Risk",
      "fibo_class": "fiboâ€‘riskâ€‘fi:InterestRateRisk",
      "description": "...",
      "context": "..."
    }
  ],
  "entities": [
    {
      "name": "Acme Bank",
      "fibo_class": "fiboâ€‘fbcâ€‘fiâ€‘fi:FinancialInstitution",
      "description": "...",
      "context": "..."
    }
  ],
  "relationships": [
    {
      "name": "Acme Bank lends to Beta Corp",
      "fibo_class": "fiboâ€‘fbcâ€‘fiâ€‘fi:LendingRelationship",
      "description": "...",
      "context": "..."
    }
  ],
  "financial_data": [
    {
      "name": "Total Assets",
      "value": "$1,234,567,890",
      "ontology_class": "crâ€‘assets:TotalAssets",
      "description": "The bankâ€™s total assets as of reporting date.",
      "context": "..."
    }
  ]
}
```

### Custom Call Report Ontology (prefix `crâ€‘`)  
- **Assets**: `crâ€‘assets:TotalAssets`, `crâ€‘assets:CashAndBalancesDue`, `crâ€‘assets:LoansAndLeases`, `crâ€‘assets:GoodwillAndIntangibles`  
- **Liabilities**: `crâ€‘liabilities:TotalLiabilities`, `crâ€‘liabilities:Deposits`, `crâ€‘liabilities:BorrowedMoney`  
- **Equity**: `crâ€‘equity:TotalEquityCapital`, `crâ€‘equity:RetainedEarnings`  
- **Income & Expenses**: `crâ€‘income:NetIncome`, `crâ€‘income:InterestIncome`, `crâ€‘expense:InterestExpense`, `crâ€‘expense:ProvisionForLoanLosses`  
- **Capital & Risk**: `crâ€‘capital:Tier1Capital`, `crâ€‘capital:RiskWeightedAssets`, `crâ€‘capital:Tier1LeverageRatio`  
- **General**: `crâ€‘general:Date`, `crâ€‘general:Percentage`, `crâ€‘general:Identifier`, `crâ€‘general:OtherMetric`

## âœ… Why This Approach  
- Enables browserâ€‘based PDF text extraction (via PDF.js) so you can run in web apps without server overhead.  
- Uses chunking logic to keep input sizes safe for largeâ€‘language models and avoid context overflow.  
- Schemaâ€‘driven prompt approach ensures output is structured and machineâ€‘readable (not just free text).  
- Deduplication ensures output is optimized and avoids redundant facts/data points.  
- Easily integrates with RAG workflows, vector stores, dashboards or other data pipelines.

## ğŸ”® Roadmap  
- Add CLI or backend service (Node.js) for batch processing of many PDFs.  
- Add advanced table extraction (detect and parse tabular data inside reports).  
- Publish full custom ontology documentation (JSON Schema, vocabulary).  
- Add additional exporters (CSV, Parquet) alongside JSON.  
- Add unit & integration tests covering: chunking logic, PDF extraction, schema validation, deduplication.  
- Possibly support file uploads + processing in serverless/cloud functions.

## ğŸ¤ Contributing  
Contributions are welcome!  
1. Fork this repository  
2. Create a new branch for your feature or bugâ€‘fix  
3. Add relevant tests where applicable  
4. Submit a pull request describing your changes, motivation and implications

## ğŸ“ License  
MIT
